{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scheduling\n",
    "\n",
    "Operations like `dask.delayed` generate task graphs where each node in the graph is a normal Python function and edges between nodes are normal Python objects that are created by one task as outputs and used as inputs in another task.\n",
    "\n",
    "After Dask generates these task graphs, it needs to execute them on parallel hardware. This is the job of a **task scheduler**.\n",
    "\n",
    "Different task schedulers exist, and each will consume a task graph and compute the same result, but with different performance characteristics.\n",
    "\n",
    "Dask has two families of task schedulers:\n",
    "\n",
    "1.  **Single machine scheduler family**: This provides basic features on a local process or thread pool.  This is the default. It is simple and cheap to use, although it can only be used on a single machine, so scalability is limitted.\n",
    "2.  **Distributed scheduler**: This scheduler is more sophisticated, offers more features, but also requires a bit more effort to set up.  It can run locally or distributed across a cluster.\n",
    "\n",
    "<img src=\"assets/collections-schedulers.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For different computations you may find better performance with particular scheduler settings. We'll explore different schedulers and their impact.\n",
    "\n",
    "Consider the following, which is similar to what we explored in the last section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import delayed\n",
    "from time import sleep\n",
    "import random\n",
    "import dask\n",
    "\n",
    "def inc(x):\n",
    "    sleep(1)\n",
    "    # Or we can make it sleep a random number less than 20\n",
    "    # sleep(random.randrange(20))\n",
    "    return x + 1\n",
    "\n",
    "data = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "results = []\n",
    "\n",
    "for x in data:\n",
    "    y = delayed(inc)(x)\n",
    "    results.append(y)\n",
    "    \n",
    "total = delayed(sum)(results)\n",
    "\n",
    "# Computing ...\n",
    "%time result = total.compute()\n",
    "print(\"Printing result from computing total:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single thread\n",
    "\n",
    "If we wanted to, we could compute the result using only a single thread. Each task in the graph is executed one-at-a-time. We do this by telling Dask to use the 'sychronous' scheduler, by passing a keyword option to the `compute` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time result = total.compute(scheduler='synchronous')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why would you want to do this? Well, most of the time you wouldn't, but this can be helpful when trying to fix code using debugging tools that don't parallelize well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Threads\n",
    "\n",
    "The threaded scheduler executes computations with a local `multiprocessing.pool.ThreadPool` (from the Python `multiprocessing` library). It is lightweight and requires no setup. It introduces very little task overhead and, because everything occurs in the same process, it incurs no costs to transfer data between tasks. However, due to Python’s Global Interpreter Lock (GIL), this scheduler only provides parallelism when your computation is dominated by non-Python code, such as is the case when operating on numeric data in NumPy arrays, Pandas DataFrames, or using any of the other C/C++/Cython based projects in the ecosystem.\n",
    "\n",
    "The threaded scheduler is the **default** choice for Dask Delayed (and Dask Array and Dask DataFrame). However, if your computation is dominated by processing pure Python objects like strings, dicts, or lists, then you may want to try one of the process-based schedulers below (we currently recommend the distributed scheduler on a local machine).\n",
    "\n",
    "Although this is the default (unless Dask is configured otherwise), we can use this scheduler by telling Dask to use the 'threads' scheduler, by passing a keyword option to the `compute` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time result = total.compute(scheduler='threads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Processes\n",
    "\n",
    "The multiprocessing scheduler executes computations with a local `multiprocessing.Pool`.\n",
    "\n",
    "It is lightweight to use and requires no setup.\n",
    "Every task and all of its dependencies are shipped to a local process,\n",
    "executed, and then their result is shipped back to the main process.\n",
    "This means that it is able to bypass issues with the GIL and provide parallelism even on computations that are dominated by pure Python code,\n",
    "such as those that process strings, dicts, and lists.\n",
    "\n",
    "However, moving data to remote processes and back can introduce performance penalties, particularly when the data being transferred between processes is large. The multiprocessing scheduler is an excellent choice when workflows are relatively linear, and so does not involve significant inter-task data transfer as well as when inputs and outputs are both small, like filenames and counts.\n",
    "\n",
    "This is common in basic data ingestion workloads, such as those are common in `Dask Bag`, where the multiprocessing scheduler is the default:\n",
    "\n",
    "```python\n",
    "# Read in a bunch of json files...\n",
    "# parse them...\n",
    "# grab all the name attributes...\n",
    "# and compute frequencies\n",
    "\n",
    "import dask.bag as db\n",
    "db.read_text('*.json').map(json.loads).pluck('name').frequencies().compute()\n",
    "\n",
    "{'alice': 100, 'bob': 200, 'charlie': 300}\n",
    "```\n",
    "\n",
    "We tell Dask to use the `threads` scheduler by passing a keyword option to the `compute` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time result = total.compute(scheduler='processes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed (local)\n",
    "\n",
    "### Note: some things won't work on Colab\n",
    "\n",
    "The Dask distributed scheduler can either be setup on a cluster or run locally on a personal machine. Despite having the name “distributed”, it is often pragmatic on local machines for a few reasons:\n",
    "\n",
    "* It provides access to asynchronous API, notably Futures\n",
    "* It provides a diagnostic dashboard that can provide valuable insight on performance and progress\n",
    "* It handles data locality with more sophistication, and so can be more efficient than the multiprocessing scheduler on workloads that require multiple processes\n",
    "\n",
    "Here's how we start a distributed local **client**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE!!! Won't work directly on Colab, see below\n",
    "\n",
    "from dask.distributed import Client\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Colab, we need to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE!!! Only for Colab, not for local Jupyter\n",
    "\n",
    "from dask.distributed import Client\n",
    "client = Client(processes=False, diagnostics_port=None)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: When we create a `Client` object it registers itself as the default Dask scheduler**. All `.compute()` methods will automatically start using the distributed system.\n",
    "\n",
    "The `Client` connects to a `Cluster`, which is a pool of workers (in software) that will execute any tasks we sent to it. On my laptop, the notebook tells me that I can use `4` workers, `8` cores, and `~17 GB` of memory.\n",
    "\n",
    "The client also gives a network address for scheduling (`tcp://127.0.0.1:64821`), and a link to a dashboard to view the execution of task graphs (`http://127.0.0.1:8787/status`).\n",
    "\n",
    "Checkout the workers tab of the dashboard: it will show you the configuration and the load on the available workers.\n",
    "\n",
    "We can now send our work to the distributed local cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time result = total.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can shutdown the cluster when we are done with it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want, we can create a `Client` that only uses a single worker and uses all of the CPUs available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(processes=False)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time result = total.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far `Client` has been creating a 'cluster' of workers for us (this is from class `LocalCluster`).\n",
    "\n",
    "We can create the cluster our selves and explicitly specify what resources we need for it, and hook up a client to the cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE!!! Won't work on Colab\n",
    "\n",
    "from distributed import Client, LocalCluster\n",
    "\n",
    "cluster = LocalCluster(n_workers=2, threads_per_worker=1)\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time result = total.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then scale up the cluster as needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(4)\n",
    "\n",
    "# The client doesn't update right away, wait a couple of seconds\n",
    "sleep(2)\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time result = total.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Distributed (Cluster)\n",
    "\n",
    "You can also run Dask on a distributed cluster. There are a variety of ways to set this up depending on your cluster.\n",
    "\n",
    "We won't get too deep into this. If you are interested, I did a webinar for WestGrid that included more information on how to use Dask with a HPC cluster: https://www.youtube.com/watch?v=uGy5gT2vLdI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colab users need something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# May need to restart your runtime after\n",
    "!pip install dask_jobqueue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example that sets up workers on an HPC cluster that uses the SLURM scheduler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_jobqueue import SLURMCluster\n",
    "from dask.distributed import Client\n",
    "from dask.distributed import progress\n",
    "import time\n",
    "\n",
    "cluster = SLURMCluster(cores=2,\n",
    "                       memory=\"8000MB\",\n",
    "                       walltime='00:30:00',\n",
    "                       project='def-blahblahblah')\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point no workers have been allocated to do work. We can checkout the job submission script that Dask uses to create a single worker (with one or more threads):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cluster.job_script())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can hook up a client to the cluster to monitor and inspect it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab: this won't work\n",
    "\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, when we want to allocate workers for our computations, we instruct Dask to submit jobs to SLURM (one job is submitted per worker):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This submits eight jobs to create workers ... this will fail on your laptop\n",
    "cluster.scale(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function doesn't wait for the workers to be scheduled -- it returns right away.\n",
    "We can wait for all of the workers to be ready with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with 8 workers ...\n",
    "while ((client.status == \"running\") and \\\n",
    "       (len(client.scheduler_info()[\"workers\"]) < 8)):\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will go through a demo of this sort of workflow at the end of this course ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to your local laptop ...\n",
    "\n",
    "You can configure the global default scheduler by using the `dask.config.set(scheduler...)` command. This can be done globally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "dask.config.set(scheduler='threads')\n",
    "\n",
    "%time result = total.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or within the context of a block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Up until here we are using the default task scheduler\n",
    "\n",
    "with dask.config.set(scheduler='processes'):\n",
    "    # Anything in this block uses the 'processes' scheduler\n",
    "    # Do stuff\n",
    "    %time result = total.compute()\n",
    "    # Do more stuff\n",
    "    \n",
    "# Outside of the block we return back to the default scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or as we have already seen, within a single compute call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time result = total.compute(scheduler='synchronous')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[On to the next notebook (`dask.array`)](04-array.ipynb) ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
